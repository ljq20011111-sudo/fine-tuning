import os
import pandas as pd
import torch
from transformers import (
    AutoTokenizer, AutoModelForCausalLM,
    Trainer, TrainingArguments,
    DataCollatorForLanguageModeling,
    EarlyStoppingCallback,TrainerCallback
)
from datasets import Dataset


class EvalEveryNEpochCallback(TrainerCallback):
    def __init__(self, n=3):
        self.n = n

    def on_epoch_end(self, args, state, control, **kwargs):
        # æ¯ n ä¸ª epoch è¿›è¡Œè¯„ä¼°
        if (state.epoch % self.n) == 0:
            control.should_evaluate = True
        else:
            control.should_evaluate = False
        return control

class ImprovedEarlyStopping(EarlyStoppingCallback):
    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        # åªè·å– eval_loss ä½œä¸ºæ—©åœæŒ‡æ ‡
        metric_value = metrics.get("eval_loss")

        if metric_value is None:
            print("âš ï¸ è¯„ä¼°ç»“æœä¸­æ²¡æœ‰æ‰¾åˆ° eval_lossï¼Œæ—©åœæ£€æŸ¥è·³è¿‡")
            return

        # è°ƒç”¨çˆ¶ç±»æ–¹æ³•æ£€æŸ¥æŒ‡æ ‡æ˜¯å¦æ”¹å–„
        self.check_metric_value(args, state, control, metric_value)

        # æ‰“å°æ—©åœè®¡æ•°å™¨çŠ¶æ€
        if self.early_stopping_patience_counter > 0:
            print(f"âš ï¸ æ—©åœè®¡æ•°å™¨: {self.early_stopping_patience_counter}/{self.early_stopping_patience}")

        # è¾¾åˆ°è€å¿ƒæ¬¡æ•°åˆ™åœæ­¢è®­ç»ƒ
        if self.early_stopping_patience_counter >= self.early_stopping_patience:
            control.should_training_stop = True



# è®­ç»ƒç»“æœåˆ†æå‡½æ•°
def analyze_training_results(trainer, output_dir):
    """åˆ†æè®­ç»ƒç»“æœï¼Œè®­ç»ƒæ›²çº¿ä¿å­˜ eval_loss å’Œ learning_rateï¼Œæœ€ç»ˆåªæ‰“å° eval_loss"""
    # è·å–è®­ç»ƒå†å²
    history = trainer.state.log_history
    df_history = pd.DataFrame(history)

    # ä¿å­˜è®­ç»ƒæ›²çº¿æ•°æ®ï¼Œåªä¿ç•™ step, epoch, eval_loss, learning_rate
    columns_to_save = [col for col in ["step", "epoch", "eval_loss", "learning_rate"] if col in df_history.columns]
    df_history[columns_to_save].to_csv(os.path.join(output_dir, "training_history.csv"), index=False)

    # æœ€ç»ˆè¯„ä¼°
    final_eval_loss = trainer.evaluate().get("eval_loss")
    print("\nğŸ“Š æœ€ç»ˆè¯„ä¼°ç»“æœ:")
    print(f"  eval_loss: {final_eval_loss:.4f}")

    # è¿”å›è®­ç»ƒæ›²çº¿ DataFrameï¼ˆå¯é€‰ï¼Œå¦‚æœåç»­ç»˜å›¾ä½¿ç”¨ï¼‰
    return df_history[columns_to_save]



if __name__ == "__main__":
    # === 1ï¸âƒ£ ç¯å¢ƒå˜é‡è®¾ç½® ===
    os.environ["HF_HOME"] = "/root/autodl-tmp/fine-tuning/huggingface"
    os.environ["TRANSFORMERS_CACHE"] = "/root/autodl-tmp/fine-tuning/huggingface"
    os.environ["HF_ENDPOINT"] = "https://huggingface.co"
    os.environ["HF_HUB_DISABLE_SSL_VERIFY"] = "1"

    output_dir = "/root/autodl-tmp/fine-tuning/protGPT2_finetuned"
    log_dir = "/root/autodl-tmp/fine-tuning/protGPT2_logs"


    # === 2ï¸âƒ£ åŠ è½½å·²ç»åˆ’åˆ†å¥½çš„è®­ç»ƒé›†å’ŒéªŒè¯é›† ===
    def load_dataset(file_path):
        """è¯»å–txtæ–‡ä»¶ä¸ºDataFrame"""
        with open(file_path, "r", encoding="utf-8") as f:
            lines = [line.strip() for line in f if line.strip()]
        return pd.DataFrame({"text": lines})


    print("ğŸ“‚ æ­£åœ¨åŠ è½½è®­ç»ƒå’ŒéªŒè¯æ•°æ®...")
    df_train = load_dataset("train.txt")
    df_val = load_dataset("val.txt")

    # è½¬ä¸º Hugging Face Dataset
    dataset_train = Dataset.from_pandas(df_train)
    dataset_val = Dataset.from_pandas(df_val)

    # === 3ï¸âƒ£ åŠ è½½æ¨¡å‹ä¸åˆ†è¯å™¨ ===
    print("ğŸŒ æ­£åœ¨åŠ è½½ ProtGPT2 æ¨¡å‹ä¸åˆ†è¯å™¨...")
    model_name = "nferruz/ProtGPT2"

    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        cache_dir="/root/autodl-tmp/fine-tuning/huggingface"
    )
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        cache_dir="/root/autodl-tmp/fine-tuning/huggingface"
    )

    # è®¾ç½®pad_tokenï¼ˆProtGPT2æ²¡æœ‰pad_tokenï¼Œéœ€è¦æ‰‹åŠ¨æŒ‡å®šï¼‰
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.model_max_length = 256
    model.config.pad_token_id = tokenizer.eos_token_id


    # === 4ï¸âƒ£ Tokenize æ•°æ®ï¼špadding=Falseï¼Œä¹Ÿå°±æ˜¯ä¸åœ¨æ•°æ®å‡†å¤‡æ—¶å¡«å……ï¼Œè€Œæ˜¯åœ¨ DataCollator æ—¶æŒ‰ batch åŠ¨æ€ paddingã€‚===
    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            truncation=True,
            max_length=256,
            padding=False
        )


    print("ğŸ”¡ æ­£åœ¨è¿›è¡Œåˆ†è¯å¤„ç†...")
    tokenized_train = dataset_train.map(tokenize_function, batched=True, remove_columns=["text"])
    tokenized_val = dataset_val.map(tokenize_function, batched=True, remove_columns=["text"])

    # === 5ï¸âƒ£ Data Collatorï¼šmlm=False â†’ å› æœè¯­è¨€æ¨¡å‹è®­ç»ƒï¼ˆCausalLMï¼‰ï¼Œæ¨¡å‹å­¦çš„æ˜¯é¢„æµ‹ä¸‹ä¸€ä¸ªæ°¨åŸºé…¸ï¼Œè€Œä¸æ˜¯æ©ç å¡«ç©º ===
    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

    # === 6ï¸âƒ£ æ˜¾å­˜ä¼˜åŒ–ï¼šå‡å°‘å‰å‘å­˜å‚¨ç»“æœï¼Œé™ä½æ˜¾å­˜å‹åŠ›ï¼Œä»£ä»·æ˜¯ç•¥å¾®é™ä½é€Ÿåº¦===
    model.gradient_checkpointing_enable()
    model.config.use_cache = False

    # === 7ï¸âƒ£ è®­ç»ƒå‚æ•° ===
    training_args = TrainingArguments(
        output_dir=output_dir,
        overwrite_output_dir=True,
        num_train_epochs=50,
        per_device_train_batch_size=4,
        gradient_accumulation_steps=8,  # æŠŠ 4*8 = 32 çš„ batch ç­‰æ•ˆè®­ç»ƒï¼Œé¿å…æ˜¾å­˜ä¸è¶³
        learning_rate=5e-5,
        warmup_steps=100,
        weight_decay=0.01,
        fp16=True,
        save_strategy="epoch",
        save_total_limit=3,
        eval_strategy="epoch",
        logging_strategy="steps",
        logging_steps=50,
        logging_dir=log_dir,
        report_to=["tensorboard"],
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss", # ä»¥éªŒè¯é›† loss ä½œä¸ºâ€œå¥½åæ ‡å‡†â€
        greater_is_better=False,
    )
    # å¦‚æœéªŒè¯é›† loss è¿ç»­ 3 æ¬¡æ²¡æœ‰ä¸‹é™ â†’ åœæ­¢è®­ç»ƒï¼Œé¿å…è¿‡æ‹Ÿåˆ
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_train,
        eval_dataset=tokenized_val,
        data_collator=data_collator,
        callbacks=[
            ImprovedEarlyStopping(early_stopping_patience=3),  # æ—©åœ
            EvalEveryNEpochCallback(n=3)  # æ¯3ä¸ªepochéªŒè¯ä¸€æ¬¡
        ]
    )

    # === ğŸ”Ÿ å¼€å§‹è®­ç»ƒ ===
    print("ğŸš€ å¼€å§‹å¾®è°ƒ ProtGPT2 æ¨¡å‹...\n")

    # è‡ªåŠ¨æ–­ç‚¹æ¢å¤
    checkpoints = [d for d in os.listdir(output_dir) if d.startswith("checkpoint")]
    if checkpoints:
        latest_checkpoint = os.path.join(output_dir, sorted(checkpoints)[-1])
        print(f"ğŸ” æ£€æµ‹åˆ°æ–­ç‚¹ {latest_checkpoint} ï¼Œä»ä¸Šæ¬¡ä¿å­˜ç‚¹æ¢å¤è®­ç»ƒ...\n")
        trainer.train(resume_from_checkpoint=latest_checkpoint)
    else:
        trainer.train()

    # === 1ï¸âƒ£1ï¸âƒ£ ä¿å­˜æœ€ç»ˆæ¨¡å‹ ===
    print("\nğŸ’¾ ä¿å­˜æ¨¡å‹ä¸åˆ†è¯å™¨ä¸­...")
    trainer.save_model(output_dir)
    tokenizer.save_pretrained(output_dir)

    # === 1ï¸âƒ£2ï¸âƒ£ åˆ†æè®­ç»ƒç»“æœ ===
    print("\nğŸ“ˆ æ­£åœ¨åˆ†æè®­ç»ƒç»“æœ...")
    training_history = analyze_training_results(trainer, output_dir)

    print(f"\nâœ… å¾®è°ƒå®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜è‡³: {output_dir}")
    print(f"ğŸ“Š TensorBoard æ—¥å¿—ç›®å½•: {log_dir}")
    print(f"ğŸ“„ è®­ç»ƒå†å²å·²ä¿å­˜è‡³: {os.path.join(output_dir, 'training_history.csv')}")
    print(f"ğŸ’¡ æŸ¥çœ‹è®­ç»ƒæ›²çº¿ï¼štensorboard --logdir=\"{log_dir}\" --port=6006")