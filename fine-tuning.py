import os
import pandas as pd
import torch
from transformers import (
    AutoTokenizer, AutoModelForCausalLM,
    Trainer, TrainingArguments,
    DataCollatorForLanguageModeling,
    EarlyStoppingCallback, TrainerCallback
)
from datasets import Dataset


class EvalEveryNEpochCallback(TrainerCallback):
    def __init__(self, n=3):
        self.n = n

    def on_epoch_end(self, args, state, control, **kwargs):
        # æ¯ n ä¸ª epoch è¿›è¡Œè¯„ä¼°
        if (state.epoch % self.n) == 0:
            control.should_evaluate = True
        else:
            control.should_evaluate = False
        return control


'''
class ImprovedEarlyStopping(EarlyStoppingCallback):
    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        # åªè·å– eval_loss ä½œä¸ºæ—©åœæŒ‡æ ‡
        metric_value = metrics.get("eval_loss")

        if metric_value is None:
            print("âš ï¸ è¯„ä¼°ç»“æœä¸­æ²¡æœ‰æ‰¾åˆ° eval_lossï¼Œæ—©åœæ£€æŸ¥è·³è¿‡")
            return

        # è°ƒç”¨çˆ¶ç±»æ–¹æ³•æ£€æŸ¥æŒ‡æ ‡æ˜¯å¦æ”¹å–„
        self.check_metric_value(args, state, control, metric_value)

        # æ‰“å°æ—©åœè®¡æ•°å™¨çŠ¶æ€
        if self.early_stopping_patience_counter > 0:
            print(f"âš ï¸ æ—©åœè®¡æ•°å™¨: {self.early_stopping_patience_counter}/{self.early_stopping_patience}")

        # è¾¾åˆ°è€å¿ƒæ¬¡æ•°åˆ™åœæ­¢è®­ç»ƒ
        if self.early_stopping_patience_counter >= self.early_stopping_patience:
            control.should_training_stop = True
'''

# è®­ç»ƒç»“æœåˆ†æå‡½æ•°
def analyze_training_results(trainer, output_dir):
    """åˆ†æè®­ç»ƒç»“æœï¼Œè®­ç»ƒæ›²çº¿ä¿å­˜ eval_loss å’Œ learning_rateï¼Œæœ€ç»ˆåªæ‰“å° eval_loss"""
    history = trainer.state.log_history
    df_history = pd.DataFrame(history)

    columns_to_save = [col for col in ["step", "epoch", "eval_loss", "learning_rate"] if col in df_history.columns]
    df_history[columns_to_save].to_csv(os.path.join(output_dir, "training_history.csv"), index=False)

    final_eval_loss = trainer.evaluate().get("eval_loss")
    print("\nğŸ“Š æœ€ç»ˆè¯„ä¼°ç»“æœ:")
    print(f"  eval_loss: {final_eval_loss:.4f}")


if __name__ == "__main__":
    # === 1ï¸âƒ£ ç¯å¢ƒå˜é‡è®¾ç½® ===
    os.environ["HF_HOME"] = "/root/autodl-tmp/fine-tuning/huggingface"
    os.environ["TRANSFORMERS_CACHE"] = "/root/autodl-tmp/fine-tuning/huggingface"
    os.environ["HF_ENDPOINT"] = "https://huggingface.co"
    os.environ["HF_HUB_DISABLE_SSL_VERIFY"] = "1"

    output_dir = "/root/autodl-tmp/fine-tuning/protGPT2_finetuned"
    log_dir = "/root/autodl-tmp/fine-tuning/protGPT2_logs"


    # === 2ï¸âƒ£ åŠ è½½è®­ç»ƒå’ŒéªŒè¯é›† ===
    def load_dataset(file_path):
        with open(file_path, "r", encoding="utf-8") as f:
            lines = [line.strip() for line in f if line.strip()]
        return pd.DataFrame({"text": lines})

    print("ğŸ“‚ æ­£åœ¨åŠ è½½è®­ç»ƒå’ŒéªŒè¯æ•°æ®...")
    df_train = load_dataset("train.txt")
    df_val = load_dataset("val.txt")

    dataset_train = Dataset.from_pandas(df_train)
    dataset_val = Dataset.from_pandas(df_val)


    # === 3ï¸âƒ£ åŠ è½½æ¨¡å‹ä¸åˆ†è¯å™¨ ===
    print("ğŸŒ æ­£åœ¨åŠ è½½ ProtGPT2 æ¨¡å‹ä¸åˆ†è¯å™¨...")
    model_name = "nferruz/ProtGPT2"

    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        cache_dir="/root/autodl-tmp/fine-tuning/huggingface"
    )
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        cache_dir="/root/autodl-tmp/fine-tuning/huggingface"
    )

    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.model_max_length = 256
    model.config.pad_token_id = tokenizer.eos_token_id


    # === 4ï¸âƒ£ Tokenize æ•°æ® ===
    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            truncation=True,
            max_length=256,
            padding=False
        )

    print("ğŸ”¡ æ­£åœ¨è¿›è¡Œåˆ†è¯å¤„ç†...")
    tokenized_train = dataset_train.map(tokenize_function, batched=True, remove_columns=["text"])
    tokenized_val = dataset_val.map(tokenize_function, batched=True, remove_columns=["text"])

    # === 5ï¸âƒ£ Data Collator ===
    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

    # === 6ï¸âƒ£ æ˜¾å­˜ä¼˜åŒ– ===
    model.gradient_checkpointing_enable()
    model.config.use_cache = False


    # === 7ï¸âƒ£ è®­ç»ƒå‚æ•° ===
    training_args = TrainingArguments(
        output_dir=output_dir,
        overwrite_output_dir=True,
        num_train_epochs=100,
        per_device_train_batch_size=4,
        gradient_accumulation_steps=8,
        learning_rate=5e-5,  # æˆ– 2e-5
        lr_scheduler_type="cosine",  # ä½™å¼¦è¡°å‡æ›´å¹³æ»‘
        warmup_steps=100,
        weight_decay=0.01,
        fp16=True,
        save_strategy="epoch",
        save_total_limit=3,
        eval_strategy="epoch",
        logging_strategy="steps",
        logging_steps=50,
        logging_dir=log_dir,
        report_to=["tensorboard"],
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_train,
        eval_dataset=tokenized_val,
        data_collator=data_collator,
        callbacks=[
            EvalEveryNEpochCallback(n=3)
        ]
    )
    # === ğŸ”Ÿ å¼€å§‹è®­ç»ƒ ===
    print("ğŸš€ å¼€å§‹å¾®è°ƒ ProtGPT2 æ¨¡å‹...\n")

    # æŒ‡å®šè¦æ¢å¤çš„æ£€æŸ¥ç‚¹
    specific_checkpoint = "/root/autodl-tmp/fine-tuning/protGPT2_finetuned/checkpoint-16089"
    if os.path.exists(specific_checkpoint):
        print(f"ğŸ” ä»æŒ‡å®šæ£€æŸ¥ç‚¹ {specific_checkpoint} æ¢å¤è®­ç»ƒ...\n")
        trainer.train(resume_from_checkpoint=specific_checkpoint)
    else:
        checkpoints = [d for d in os.listdir(output_dir) if d.startswith("checkpoint")]
        if checkpoints:
            latest_checkpoint = os.path.join(output_dir, sorted(checkpoints)[-1])
            print(f"ğŸ” æ£€æµ‹åˆ°æ–­ç‚¹ {latest_checkpoint} ï¼Œä»ä¸Šæ¬¡ä¿å­˜ç‚¹æ¢å¤è®­ç»ƒ...\n")
            trainer.train(resume_from_checkpoint=latest_checkpoint)
        else:
            trainer.train()

    # === 1ï¸âƒ£1ï¸âƒ£ ä¿å­˜æœ€ç»ˆæ¨¡å‹ ===
    print("\nğŸ’¾ ä¿å­˜æ¨¡å‹ä¸åˆ†è¯å™¨ä¸­...")
    trainer.save_model(output_dir)
    tokenizer.save_pretrained(output_dir)

    # === 1ï¸âƒ£2ï¸âƒ£ åˆ†æè®­ç»ƒç»“æœ ===
    print("\nğŸ“ˆ æ­£åœ¨åˆ†æè®­ç»ƒç»“æœ...")
    training_history = analyze_training_results(trainer, output_dir)

    print(f"\nâœ… å¾®è°ƒå®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜è‡³: {output_dir}")
    print(f"ğŸ“Š TensorBoard æ—¥å¿—ç›®å½•: {log_dir}")
    print(f"ğŸ“„ è®­ç»ƒå†å²å·²ä¿å­˜è‡³: {os.path.join(output_dir, 'training_history.csv')}")
    print(f"ğŸ’¡ æŸ¥çœ‹è®­ç»ƒæ›²çº¿ï¼štensorboard --logdir=\"{log_dir}\" --port=6006")
